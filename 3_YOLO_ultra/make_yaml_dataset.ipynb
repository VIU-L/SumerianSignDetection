{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "075617a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transforms current dataset structure to YOLO format\n",
    "import os\n",
    "import shutil\n",
    "import json\n",
    "ROOT_DIR = \"D:/sumer/\"\n",
    "OUTPUT_DIR = \"D:/sumer/3_YOLO_ultra/dataset/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e632f69f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# current structure :\n",
    "# ROOT_DIR/HeiCuBeDa/Images_MSII_Filter/  --> images\n",
    "# MaiCuBeDa/train_photo_anno.json  --> annotations (train)\n",
    "# MaiCuBeDa/test_photo_anno.json --> annotations (val)\n",
    "\n",
    "# content in json:\n",
    "# {\n",
    "#     \"HS_1220_03\": {\n",
    "#         \"tablet_ID\": \"HS_1220\",\n",
    "#         \"side\": \"front\",\n",
    "#         \"image_path\": \"../HeiCuBeDa/Images_MSII_Filter/HS_1220_HeiCuBeDa_GMOCF_r1.50_n4_v512_03_front.png\",\n",
    "#         \"bboxes\": [\n",
    "#             {\n",
    "#                 \"bbox\": [\n",
    "#                     13,\n",
    "#                     17,\n",
    "#                     121,\n",
    "#                     122\n",
    "#                 ],\n",
    "#                 \"charname\": \"U\",\n",
    "#                 \"transliteration\": \"1(u)\",\n",
    "#                 \"charname_id\": 17,\n",
    "#                 \"transliteration_id\": 21\n",
    "#             },\n",
    "#             {\n",
    "#                 \"bbox\": [\n",
    "#                     11,\n",
    "#                     159,\n",
    "#                     111,\n",
    "#                     293\n",
    "#                 ],\n",
    "#                 \"charname\": \"KI\",\n",
    "#                 \"transliteration\": \"ki\",\n",
    "#                 \"charname_id\": 3,\n",
    "#                 \"transliteration_id\": 3\n",
    "#             },..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8fa3e20a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# required YOLO format:\n",
    "# OUTPUT_DIR/images/train/  --> images for training\n",
    "# OUTPUT_DIR/images/val/    --> images for validation\n",
    "# OUTPUT_DIR/labels/train/  --> labels for training\n",
    "# OUTPUT_DIR/labels/val/    --> labels for validation\n",
    "# where label files are .txt files with same name as images, each line in txt file:\n",
    "# <class_id> <x_center> <y_center> <width> <height>\n",
    "# all values are normalized (divided by image width and height)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c25e7c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# existing files\n",
    "TRAIN_JSON = os.path.join(ROOT_DIR, \"MaiCuBeDa/train_photo_anno.json\")\n",
    "VAL_JSON   = os.path.join(ROOT_DIR, \"MaiCuBeDa/test_photo_anno.json\")\n",
    "IMAGES_DIR = os.path.join(ROOT_DIR, \"HeiCuBeDa/Images_MSII_Filter/\")\n",
    "\n",
    "# choose task\n",
    "label_key = \"charname_topN\" # \"is_sign\" / \"charname\" / \"transliteration\" / \"charname_topN\"\n",
    "N=150\n",
    "if label_key != \"is_sign\":\n",
    "    OUTPUT_DIR = OUTPUT_DIR[:-1] + f\"_{label_key}\"\n",
    "\n",
    "# target YOLO folders\n",
    "IMG_TRAIN_DIR = os.path.join(OUTPUT_DIR, \"images/train\")\n",
    "IMG_VAL_DIR   = os.path.join(OUTPUT_DIR, \"images/val\")\n",
    "LBL_TRAIN_DIR = os.path.join(OUTPUT_DIR, \"labels/train\")\n",
    "LBL_VAL_DIR   = os.path.join(OUTPUT_DIR, \"labels/val\")\n",
    "os.makedirs(IMG_TRAIN_DIR, exist_ok=True)\n",
    "os.makedirs(IMG_VAL_DIR, exist_ok=True)\n",
    "os.makedirs(LBL_TRAIN_DIR, exist_ok=True)\n",
    "os.makedirs(LBL_VAL_DIR, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fca37b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "if label_key == \"charname\" or label_key == \"charname_topN\":\n",
    "    with open(os.path.join(ROOT_DIR, \"MaiCuBeDa/charname_to_id.json\"),\"r\",encoding=\"utf-8\") as f:\n",
    "        dict_id = json.load(f)\n",
    "    if label_key == \"charname_topN\":\n",
    "        for k,v in dict_id.items():\n",
    "            if v >= N:\n",
    "                dict_id[k] = N  # group together\n",
    "        # save modified dict\n",
    "        with open(os.path.join(ROOT_DIR, f\"MaiCuBeDa/charname_to_id_top{N}.json\"),\"w\",encoding=\"utf-8\") as f:\n",
    "            json.dump(dict_id, f, ensure_ascii=False, indent=4)\n",
    "elif label_key == \"transliteration\":\n",
    "    with open(os.path.join(ROOT_DIR, \"MaiCuBeDa/transliteration_to_id.json\"),\"r\",encoding=\"utf-8\") as f:\n",
    "        dict_id = json.load(f)\n",
    "        \n",
    "reverse_dict_id = {v:k for k,v in dict_id.items()}\n",
    "if label_key == \"charname_topN\":\n",
    "    reverse_dict_id[N] = \"OTHER\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "215b3e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_bbox(bbox, img_width, img_height):\n",
    "    x_min, y_min, x_max, y_max = bbox\n",
    "    x_center = (x_min + x_max) / 2.0 / img_width\n",
    "    y_center = (y_min + y_max) / 2.0 / img_height\n",
    "    width    = (x_max - x_min) / img_width\n",
    "    height   = (y_max - y_min) / img_height\n",
    "    return x_center, y_center, width, height"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e5907c9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing training data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing items: 100%|██████████| 856/856 [00:03<00:00, 259.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing validation data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing items: 100%|██████████| 214/214 [00:00<00:00, 258.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done! YOLO dataset ready.\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "def process_json(json_file, img_output_dir, lbl_output_dir, label_key):\n",
    "    with open(json_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    for key, item in tqdm(data.items(), desc=\"Processing items\"):\n",
    "        # get image path\n",
    "        img_path = os.path.join(IMAGES_DIR, os.path.basename(item['image_path']))\n",
    "        if not os.path.exists(img_path):\n",
    "            print(f\"Warning: {img_path} does not exist.\")\n",
    "            continue\n",
    "        \n",
    "        # copy image\n",
    "        dst_img_path = os.path.join(img_output_dir, os.path.basename(img_path))\n",
    "        shutil.copy(img_path, dst_img_path)\n",
    "\n",
    "        # open image to get size\n",
    "        with Image.open(img_path) as img:\n",
    "            w, h = img.size\n",
    "        \n",
    "        # write label file\n",
    "        label_file = os.path.splitext(os.path.basename(img_path))[0] + \".txt\"\n",
    "        label_file_path = os.path.join(lbl_output_dir, label_file)\n",
    "        \n",
    "        with open(label_file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            for bbox_entry in item['bboxes']:\n",
    "                if label_key == \"is_sign\":\n",
    "                    class_id = 0\n",
    "                elif label_key == \"charname\":\n",
    "                    class_id = bbox_entry['charname_id']\n",
    "                elif label_key == \"transliteration\":\n",
    "                    class_id = bbox_entry['transliteration_id']\n",
    "                elif label_key == \"charname_topN\":\n",
    "                    class_id = bbox_entry['charname_id']\n",
    "                    if class_id >= N:\n",
    "                        class_id = N  # group together\n",
    "                else:\n",
    "                    raise ValueError(\"label_key must be one of ['is_sign','charname','transliteration','charname_topN']\")\n",
    "                \n",
    "                x_center, y_center, width, height = convert_bbox(bbox_entry['bbox'], w, h)\n",
    "                f.write(f\"{class_id} {x_center:.6f} {y_center:.6f} {width:.6f} {height:.6f}\\n\")\n",
    "\n",
    "# ----------------- PROCESS DATA -----------------\n",
    "print(\"Processing training data...\")\n",
    "process_json(TRAIN_JSON, IMG_TRAIN_DIR, LBL_TRAIN_DIR, label_key)\n",
    "\n",
    "print(\"Processing validation data...\")\n",
    "process_json(VAL_JSON, IMG_VAL_DIR, LBL_VAL_DIR, label_key)\n",
    "\n",
    "print(\"Done! YOLO dataset ready.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6b0db970",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YOLO dataset.yaml saved to D:/sumer/3_YOLO_ultra/dataset_charname_top_charname_topN\\dataset.yaml\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "yaml_data = {\n",
    "    'path': OUTPUT_DIR,  # root dataset folder\n",
    "    'train': 'images/train',\n",
    "    'val': 'images/val',\n",
    "    'nc':  None,  # number of classes, will fill automatically\n",
    "    'names': None  # list of class names\n",
    "}\n",
    "if label_key == \"is_sign\":\n",
    "    yaml_data['nc'] = 1\n",
    "    yaml_data['names'] = [\"sign\"]\n",
    "else:\n",
    "    # collect all ids from all json\n",
    "    import json\n",
    "    train_json = os.path.join(\"D:/sumer/MaiCuBeDa/all_photo_anno.json\")\n",
    "    with open(train_json, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    class_ids = set()\n",
    "    for item in data.values():\n",
    "        for bbox_entry in item['bboxes']:\n",
    "            if label_key == \"charname\":\n",
    "                class_ids.add(bbox_entry['charname_id'])\n",
    "            elif label_key == \"transliteration\":\n",
    "                class_ids.add(bbox_entry['transliteration_id'])\n",
    "            elif label_key == \"charname_topN\":\n",
    "                cid = bbox_entry['charname_id']\n",
    "                if cid >= N:\n",
    "                    cid = N\n",
    "                class_ids.add(cid)\n",
    "    \n",
    "    class_ids = sorted(list(class_ids))\n",
    "    yaml_data['nc'] = len(class_ids)  # assuming class ids are 1-indexed\n",
    "    yaml_data['names'] = [reverse_dict_id[i] for i in class_ids]\n",
    "\n",
    "# ----------------- SAVE YAML -----------------\n",
    "yaml_path = os.path.join(OUTPUT_DIR, \"dataset.yaml\")\n",
    "with open(yaml_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    yaml.dump(yaml_data, f)\n",
    "\n",
    "print(f\"YOLO dataset.yaml saved to {yaml_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "080113fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove .cache files in labels/\n",
    "import glob\n",
    "for cache_file in glob.glob(os.path.join(OUTPUT_DIR, \"labels/*.cache\"), recursive=True):\n",
    "    os.remove(cache_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ccf289",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sumer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
